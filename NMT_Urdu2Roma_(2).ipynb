{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XCpF5-Zl_TO",
        "outputId": "53edff06-e5d7-4a46-8848-d49ee457527c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.8.0+cu126\n",
            "CUDA available? True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"CUDA available?\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0l4mR49YZtT",
        "outputId": "8eba68dc-f54d-41ea-fc86-6d5f586fb0ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /content/drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JQ0_Ai6acR3",
        "outputId": "ab671bc6-5980-4733-8099-681d12c840ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/25F-7801\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "PROJECT = Path(\"/content/drive/MyDrive/25F-7801\")\n",
        "print(PROJECT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO4Dmbz6bi9x",
        "outputId": "79022d8e-5c11-4642-9cd4-359d06f55132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folders created under: /content/drive/MyDrive/25F-7801\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "subdirs = [\n",
        "    \"data/raw\", \"data/processed\",\n",
        "    \"src/bpe\", \"src/model\",\n",
        "    \"checkpoints\", \"results\", \"docs\"\n",
        "]\n",
        "\n",
        "for sd in subdirs:\n",
        "    (PROJECT/sd).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Folders created under:\", PROJECT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmEVFTXRbtNo",
        "outputId": "e3b1049e-f4fd-4380-beb7-3a173b4de8dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  data/\n",
            "  src/\n",
            "  checkpoints/\n",
            "  results/\n",
            "  docs/\n",
            "    raw/\n",
            "    processed/\n",
            "    bpe/\n",
            "    model/\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "for path in PROJECT.rglob(\"*\"):\n",
        "    if path.is_dir():\n",
        "        level = len(path.relative_to(PROJECT).parts)\n",
        "        print(\"  \" * level + path.name + \"/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Htzb-Twbyod",
        "outputId": "d1afb0e7-f531-4875-b714-87e949e5e928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/25F-7801\n",
            "total 20\n",
            "drwx------ 2 root root 4096 Sep 21 11:45 checkpoints\n",
            "drwx------ 4 root root 4096 Sep 21 11:45 data\n",
            "drwx------ 2 root root 4096 Sep 21 11:45 docs\n",
            "drwx------ 2 root root 4096 Sep 21 11:45 results\n",
            "drwx------ 4 root root 4096 Sep 21 11:45 src\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir(PROJECT)\n",
        "!pwd\n",
        "!ls -la\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwgdLx11dpk8",
        "outputId": "0a73e515-d6b3-4b39-f953-732f170b5014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'urdu_ghazals_rekhta'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 112 (delta 7), reused 6 (delta 6), pack-reused 103 (from 1)\u001b[K\n",
            "Receiving objects: 100% (112/112), 2.03 MiB | 19.64 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/amir9ume/urdu_ghazals_rekhta.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPeDDBIUdtoN",
        "outputId": "4fe3ef3c-7c6d-47de-9aa5-f27fbfb4da81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "urdu_ghazals_rekhta:\n",
            "dataset\n",
            "LICENSE\n",
            "README.md\n",
            "rekhta_parser.ipynb\n",
            "sample_dataset\n",
            "\n",
            "urdu_ghazals_rekhta/dataset:\n",
            "dataset.zip\n",
            "README.md\n",
            "\n",
            "urdu_ghazals_rekhta/sample_dataset:\n",
            "english_transliteration\n",
            "hi-hazaaron-khvaahishen-aisii-ki-har-khvaahish-pe-dam-nikle-mirza-ghalib-ghazals\n",
            "ur-hazaaron-khvaahishen-aisii-ki-har-khvaahish-pe-dam-nikle-mirza-ghalib-ghazals\n"
          ]
        }
      ],
      "source": [
        "!ls -R urdu_ghazals_rekhta | head -50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LNTf8D1d3yN",
        "outputId": "4b2811f8-f6d7-40b2-db34-e963adc72222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/urdu_ghazals_rekhta'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 12 (delta 0), reused 8 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (12/12), 1.94 MiB | 5.29 MiB/s, done.\n",
            "Copied to: /content/drive/MyDrive/25F-7801/data/raw\n"
          ]
        }
      ],
      "source": [
        "# GitHub se dataset clone\n",
        "!rm -rf /content/urdu_ghazals_rekhta\n",
        "!git clone --depth 1 https://github.com/amir9ume/urdu_ghazals_rekhta /content/urdu_ghazals_rekhta\n",
        "\n",
        "# raw mein copy\n",
        "import shutil, os\n",
        "shutil.rmtree(os.path.join(PROJECT, \"data/raw\"), ignore_errors=True)\n",
        "shutil.copytree(\"/content/urdu_ghazals_rekhta\", os.path.join(PROJECT, \"data/raw\"))\n",
        "print(\"Copied to:\", os.path.join(PROJECT, \"data/raw\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "J0fgJI3BhaRG"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/drive/MyDrive/25F-7801/data/raw/dataset/dataset.zip -d /content/drive/MyDrive/25F-7801/data/raw/dataset/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oqVwampk6Tj",
        "outputId": "7ea8c4b7-7c47-434b-9315-6cc96288424e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/25F-7801/data/raw/dataset:\n",
            "dataset\n",
            "dataset.zip\n",
            "__MACOSX\n",
            "README.md\n",
            "\n",
            "/content/drive/MyDrive/25F-7801/data/raw/dataset/dataset:\n",
            "ahmad-faraz\n",
            "akbar-allahabadi\n",
            "allama-iqbal\n",
            "altaf-hussain-hali\n",
            "ameer-khusrau\n",
            "bahadur-shah-zafar\n",
            "dagh-dehlvi\n",
            "fahmida-riaz\n",
            "faiz-ahmad-faiz\n",
            "firaq-gorakhpuri\n",
            "gulzar\n",
            "habib-jalib\n",
            "jaan-nisar-akhtar\n",
            "jaun-eliya\n",
            "javed-akhtar\n",
            "jigar-moradabadi\n",
            "kaifi-azmi\n",
            "meer-anees\n",
            "meer-taqi-meer\n",
            "mirza-ghalib\n",
            "mohsin-naqvi\n",
            "naji-shakir\n",
            "naseer-turabi\n",
            "nazm-tabatabai\n",
            "nida-fazli\n",
            "noon-meem-rashid\n",
            "parveen-shakir\n",
            "sahir-ludhianvi\n",
            "wali-mohammad-wali\n",
            "waseem-barelvi\n",
            "\n",
            "/content/drive/MyDrive/25F-7801/data/raw/dataset/dataset/ahmad-faraz:\n",
            "en\n",
            "hi\n",
            "ur\n",
            "\n",
            "/content/drive/MyDrive/25F-7801/data/raw/dataset/dataset/ahmad-faraz/en:\n",
            "aankh-se-duur-na-ho-dil-se-utar-jaaegaa-ahmad-faraz-ghazals\n",
            "aashiqii-men-miir-jaise-khvaab-mat-dekhaa-karo-ahmad-faraz-ghazals\n",
            "ab-aur-kyaa-kisii-se-maraasim-badhaaen-ham-ahmad-faraz-ghazals\n",
            "abhii-kuchh-aur-karishme-gazal-ke-dekhte-hain-ahmad-faraz-ghazals\n",
            "ab-ke-ham-bichhde-to-shaayad-kabhii-khvaabon-men-milen-ahmad-faraz-ghazals\n",
            "ab-ke-tajdiid-e-vafaa-kaa-nahiin-imkaan-jaanaan-ahmad-faraz-ghazals\n"
          ]
        }
      ],
      "source": [
        "!ls -R /content/drive/MyDrive/25F-7801/data/raw/dataset| head -50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQyZOYdylH8T",
        "outputId": "9ef97297-2ff4-4bd0-d569-3f4671be04cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/25F-7801/data/raw/dataset/dataset/ahmad-faraz:\n",
            "en\n",
            "hi\n",
            "ur\n",
            "\n",
            "/content/drive/MyDrive/25F-7801/data/raw/dataset/dataset/ahmad-faraz/en:\n",
            "aankh-se-duur-na-ho-dil-se-utar-jaaegaa-ahmad-faraz-ghazals\n",
            "aashiqii-men-miir-jaise-khvaab-mat-dekhaa-karo-ahmad-faraz-ghazals\n",
            "ab-aur-kyaa-kisii-se-maraasim-badhaaen-ham-ahmad-faraz-ghazals\n",
            "abhii-kuchh-aur-karishme-gazal-ke-dekhte-hain-ahmad-faraz-ghazals\n",
            "ab-ke-ham-bichhde-to-shaayad-kabhii-khvaabon-men-milen-ahmad-faraz-ghazals\n",
            "ab-ke-tajdiid-e-vafaa-kaa-nahiin-imkaan-jaanaan-ahmad-faraz-ghazals\n",
            "ab-kyaa-sochen-kyaa-haalaat-the-kis-kaaran-ye-zahr-piyaa-hai-ahmad-faraz-ghazals\n",
            "ab-shauq-se-ki-jaan-se-guzar-jaanaa-chaahiye-ahmad-faraz-ghazals\n",
            "agarche-zor-havaaon-ne-daal-rakkhaa-hai-ahmad-faraz-ghazals\n",
            "aisaa-hai-ki-sab-khvaab-musalsal-nahiin-hote-ahmad-faraz-ghazals\n",
            "aise-chup-hain-ki-ye-manzil-bhii-kadii-ho-jaise-ahmad-faraz-ghazals\n",
            "ajab-junuun-e-masaafat-men-ghar-se-niklaa-thaa-ahmad-faraz-ghazals\n",
            "avval-avval-kii-dostii-hai-abhii-ahmad-faraz-ghazals\n",
            "dost-ban-kar-bhii-nahiin-saath-nibhaane-vaalaa-ahmad-faraz-ghazals\n",
            "dukh-fasaana-nahiin-ki-tujh-se-kahen-ahmad-faraz-ghazals\n",
            "guftuguu-achchhii-lagii-zauq-e-nazar-achchhaa-lagaa-ahmad-faraz-ghazals\n",
            "har-koii-dil-kii-hathelii-pe-hai-sahraa-rakkhe-ahmad-faraz-ghazals\n",
            "havaa-ke-zor-se-pindaar-e-baam-o-dar-bhii-gayaa-ahmad-faraz-ghazals\n",
            "huii-hai-shaam-to-aankhon-men-bas-gayaa-phir-tuu-ahmad-faraz-ghazals\n",
            "is-qadar-musalsal-thiin-shiddaten-judaaii-kii-ahmad-faraz-ghazals\n",
            "is-se-pahle-ki-be-vafaa-ho-jaaen-ahmad-faraz-ghazals\n",
            "jab-bhii-dil-khol-ke-roe-honge-ahmad-faraz-ghazals\n",
            "jis-samt-bhii-dekhuun-nazar-aataa-hai-ki-tum-ho-ahmad-faraz-ghazals\n",
            "jo-gair-the-vo-isii-baat-par-hamaare-hue-ahmad-faraz-ghazals\n",
            "juz-tire-koii-bhii-din-raat-na-jaane-mere-ahmad-faraz-ghazals\n",
            "karuun-na-yaad-magar-kis-tarah-bhulaauun-use-ahmad-faraz-ghazals\n",
            "kathin-hai-raahguzar-thodii-duur-saath-chalo-ahmad-faraz-ghazals-1\n",
            "khaamosh-ho-kyuun-daad-e-jafaa-kyuun-nahiin-dete-ahmad-faraz-ghazals\n",
            "kyaa-aise-kam-sukhan-se-koii-guftuguu-kare-ahmad-faraz-ghazals\n",
            "main-to-maqtal-men-bhii-qismat-kaa-sikandar-niklaa-ahmad-faraz-ghazals\n",
            "phir-usii-rahguzaar-par-shaayad-ahmad-faraz-ghazals\n",
            "qurbat-bhii-nahiin-dil-se-utar-bhii-nahiin-jaataa-ahmad-faraz-ghazals\n",
            "qurbaton-men-bhii-judaaii-ke-zamaane-maange-ahmad-faraz-ghazals\n",
            "ranjish-hii-sahii-dil-hii-dukhaane-ke-liye-aa-ahmad-faraz-ghazals\n",
            "rog-aise-bhii-gam-e-yaar-se-lag-jaate-hain-ahmad-faraz-ghazals\n",
            "saamne-us-ke-kabhii-us-kii-sataaish-nahiin-kii-ahmad-faraz-ghazals\n",
            "saaqiyaa-ek-nazar-jaam-se-pahle-pahle-ahmad-faraz-ghazals\n",
            "shoala-thaa-jal-bujhaa-huun-havaaen-mujhe-na-do-ahmad-faraz-ghazals\n",
            "silsile-tod-gayaa-vo-sabhii-jaate-jaate-ahmad-faraz-ghazals\n",
            "sunaa-hai-log-use-aankh-bhar-ke-dekhte-hain-ahmad-faraz-ghazals\n",
            "tere-qariib-aa-ke-badii-uljhanon-men-huun-ahmad-faraz-ghazals\n",
            "terii-baaten-hii-sunaane-aae-ahmad-faraz-ghazals-3\n",
            "tujhe-hai-mashq-e-sitam-kaa-malaal-vaise-hii-ahmad-faraz-ghazals\n",
            "tujh-se-mil-kar-to-ye-lagtaa-hai-ki-ai-ajnabii-dost-ahmad-faraz-ghazals\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!ls -R /content/drive/MyDrive/25F-7801/data/raw/dataset/dataset/ahmad-faraz | head -50\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0adBAEeBxvL",
        "outputId": "710c63cf-5a8e-408c-853e-8870ac760a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/25F-7801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKVX_hhWlNhD",
        "outputId": "c0faff0a-45cd-4e5b-c266-d41ffecb7fce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Poets found: 30\n",
            "[PAIR preview]\n",
            "  \"پھر\" -> phir\n",
            "  \"کسی\" -> kisī\n",
            "  \"کام\" -> kaam\n",
            "  \"کا\" -> kā\n",
            "  \"باقی\" -> baaqī\n",
            "  \"نہیں\" -> nahīñ\n",
            "  \"رہتا\" -> rahtā\n",
            "  \"انساں\" -> insāñ\n",
            "[PAIR preview]\n",
            "  \"جو\" -> jo\n",
            "  \"ذکر\" -> zikr\n",
            "  \"آتا\" -> aatā\n",
            "  \"ہے\" -> hai\n",
            "  \"آخرت\" -> āḳhirat\n",
            "  \"کا\" -> kā\n",
            "  \"تو\" -> to\n",
            "  \"آپ\" -> aap\n",
            "  \"ہوتے\" -> hote\n",
            "  \"ہیں\" -> haiñ\n",
            "[PAIR preview]\n",
            "  \"وہ\" -> vo\n",
            "  \"کون\" -> kaun\n",
            "  \"تھا\" -> thā\n",
            "  \"کہ\" -> ki\n",
            "  \"تمہیں\" -> tumheñ\n",
            "  \"جس\" -> jis\n",
            "  \"نے\" -> ne\n",
            "  \"بے\" -> bevafā\n",
            "  \"وفا\" -> jaanā\n",
            "[PAIR preview]\n",
            "  \"اسی\" -> isī\n",
            "  \"سے\" -> se\n",
            "  \"تو\" -> to\n",
            "  \"سر\" -> sar\n",
            "  \"آنکھوں\" -> āñkhoñ\n",
            "  \"پر\" -> par\n",
            "  \"مرا\" -> mirā\n",
            "  \"دیوان\" -> dīvān\n",
            "  \"لیتے\" -> lete\n",
            "  \"ہیں\" -> haiñ\n",
            "[PAIR preview]\n",
            "  \"شام\" -> shaam\n",
            "  \"سے\" -> se\n",
            "  \"آنکھ\" -> aañkh\n",
            "  \"میں\" -> meñ\n",
            "  \"نمی\" -> namī\n",
            "  \"سی\" -> sī\n",
            "  \"ہے\" -> hai\n",
            "[PAIR preview]\n",
            "  \"ایذا\" -> īzā\n",
            "  \"دہی\" -> dahī\n",
            "  \"کی\" -> kī\n",
            "  \"داد\" -> daad\n",
            "  \"جو\" -> jo\n",
            "  \"پاتا\" -> paatā\n",
            "  \"رہا\" -> rahā\n",
            "  \"ہوں\" -> huuñ\n",
            "  \"میں\" -> maiñ\n",
            "[PAIR preview]\n",
            "  \"آتی\" -> aatī\n",
            "  \"ہے\" -> hai\n",
            "  \"ہم\" -> ham\n",
            "  \"کو\" -> ko\n",
            "  \"شرم\" -> sharm\n",
            "  \"کہ\" -> ki\n",
            "  \"پیہم\" -> paiham\n",
            "  \"ملے\" -> mile\n",
            "  \"تمہیں\" -> tumheñ\n",
            "[PAIR preview]\n",
            "  \"چھیڑا\" -> chheḍā\n",
            "  \"تھا\" -> thā\n",
            "  \"جسے\" -> jise\n",
            "  \"پہلے\" -> pahle\n",
            "  \"پہل\" -> pahal\n",
            "  \"تیری\" -> terī\n",
            "  \"نظر\" -> nazar\n",
            "  \"نے\" -> ne\n",
            "[PAIR preview]\n",
            "  \"میں\" -> maiñ\n",
            "  \"گیا\" -> gayā\n",
            "  \"وقت\" -> vaqt\n",
            "  \"نہیں\" -> nahīñ\n",
            "  \"ہوں\" -> huuñ\n",
            "  \"کہ\" -> ki\n",
            "  \"پھر\" -> phir\n",
            "  \"آ\" -> aa\n",
            "  \"بھی\" -> bhī\n",
            "  \"نہ\" -> na\n",
            "[PAIR preview]\n",
            "  \"آپ\" -> aap\n",
            "  \"کی\" -> kī\n",
            "  \"آنکھ\" -> aañkh\n",
            "  \"سے\" -> se\n",
            "  \"گہرا\" -> gahrā\n",
            "  \"ہے\" -> hai\n",
            "  \"مری\" -> mirī\n",
            "  \"روح\" -> ruuh\n",
            "  \"کا\" -> kā\n",
            "  \"زخم\" -> zaḳhm\n",
            "[PAIR preview]\n",
            "  \"ایسے\" -> aise\n",
            "  \"کوئی\" -> koī\n",
            "  \"طوفان\" -> tūfān\n",
            "  \"ہلا\" -> hilā\n",
            "  \"بھی\" -> bhī\n",
            "  \"نہیں\" -> nahīñ\n",
            "  \"سکتا\" -> saktā\n",
            "Total line pairs collected: 21003\n",
            "Saved splits: /content/drive/MyDrive/25F-7801/data/processed sizes: 10501 5250 5252\n",
            "[BPE] Done. merges=143 vocab=200\n",
            "[BPE] Done. merges=157 vocab=200\n",
            "\n",
            "Sample Encoding:\n",
            "Urdu: فیضؔ ان کو ہے تقاضائے وفا ہم سے جنہیں\n",
            "[ENC] word=فیضؔ -> pieces=['▁ف', 'ی', 'ض', 'ؔ']\n",
            "[ENC] word=ان -> pieces=['▁ان']\n",
            "[ENC] word=کو -> pieces=['▁کو']\n",
            "[ENC] word=ہے -> pieces=['▁ہے']\n",
            "[ENC] word=تقاضائے -> pieces=['▁ت', 'ق', 'ا', 'ض', 'ا', 'ئے']\n",
            "[ENC] word=وفا -> pieces=['▁و', 'ف', 'ا']\n",
            "[ENC] word=ہم -> pieces=['▁ہم']\n",
            "[ENC] word=سے -> pieces=['▁سے']\n",
            "[ENC] word=جنہیں -> pieces=['▁ج', 'نہ', 'یں']\n",
            "[ENC] ids=[1, 126, 53, 27, 7, 154, 78, 68, 64, 33, 13, 27, 13, 96, 74, 32, 13, 106, 81, 67, 183, 60, 2]\n",
            "Roman: 'faiz' un ko hai taqāzā e vafā ham se jinheñ\n",
            "[ENC] word='faiz' -> pieces=[\"▁'\", 'f', 'a', 'i', 'z', \"'\"]\n",
            "[ENC] word=un -> pieces=['▁u', 'n']\n",
            "[ENC] word=ko -> pieces=['▁ko']\n",
            "[ENC] word=hai -> pieces=['▁hai']\n",
            "[ENC] word=taqāzā -> pieces=['▁t', 'a', 'q', 'ā', 'z', 'ā']\n",
            "[ENC] word=e -> pieces=['▁e']\n",
            "[ENC] word=vafā -> pieces=['▁v', 'a', 'f', 'ā']\n",
            "[ENC] word=ham -> pieces=['▁ham']\n",
            "[ENC] word=se -> pieces=['▁se']\n",
            "[ENC] word=jinheñ -> pieces=['▁j', 'in', 'h', 'eñ']\n",
            "[ENC] ids=[1, 171, 13, 8, 16, 33, 5, 79, 21, 71, 52, 49, 8, 24, 35, 33, 35, 50, 75, 8, 13, 35, 108, 74, 57, 114, 15, 61, 2]\n",
            "[PAIR preview]\n",
            "  \"فیضؔ\" -> 'faiz'\n",
            "  \"ان\" -> un\n",
            "  \"کو\" -> ko\n",
            "  \"ہے\" -> hai\n",
            "  \"تقاضائے\" -> taqāzā\n",
            "  \"وفا\" -> e\n",
            "  \"ہم\" -> vafā\n",
            "  \"سے\" -> ham\n",
            "  \"جنہیں\" -> se\n"
          ]
        }
      ],
      "source": [
        "# ===== preprocessing.py =====\n",
        "from pathlib import Path\n",
        "import re, unicodedata, random, shutil\n",
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "# ---- Project Paths ----\n",
        "PROJECT = Path(\"/content/drive/MyDrive/25F-7801\")\n",
        "RAW_BASE = PROJECT/\"data/raw\"\n",
        "RAW_ROOT = RAW_BASE/\"dataset/dataset\"\n",
        "OUT = PROJECT/\"data/processed\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---- Config ----\n",
        "# If True, isolated roman izāfa tokens like \"e\" or \"i\" will be removed from roman text.\n",
        "# Set to False to keep them (safer).\n",
        "REMOVE_IZAF = False\n",
        "\n",
        "# ---- Normalization ----\n",
        "_tatweel = re.compile(r\"[ـ]+\")\n",
        "_zw_chars = \"[\\u200C\\u200D]\"\n",
        "_ur_space = re.compile(r\"\\s+\")\n",
        "\n",
        "# hyphen variants and splitting regex (used consistently everywhere)\n",
        "_hyphen_variants = \"[-‐-–—]+\"   # ASCII hyphen + some unicode variants\n",
        "_hyphen_re = re.compile(_hyphen_variants)\n",
        "_split_on_hyphen_or_space = re.compile(r\"\\s+|\"+_hyphen_variants)\n",
        "\n",
        "URDU_MAP = {\n",
        "    \"\\u064A\": \"\\u06CC\",  # ARABIC YEH → FARSI YEH\n",
        "    \"\\u0643\": \"\\u06A9\",  # ARABIC KAF → KEHEH\n",
        "}\n",
        "\n",
        "def normalize_urdu(s: str) -> str:\n",
        "    if not s:\n",
        "        return s\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    s = \"\".join(URDU_MAP.get(ch, ch) for ch in s)\n",
        "    s = _tatweel.sub(\"\", s)\n",
        "    s = re.sub(_zw_chars, \"\", s)\n",
        "    s = _ur_space.sub(\" \", s.strip())\n",
        "    return s\n",
        "\n",
        "def normalize_roman(s: str) -> str:\n",
        "    if not s:\n",
        "        return s\n",
        "    s = s.strip().lower()\n",
        "    # convert hyphens to spaces so \"karam-e-be\" -> \"karam e be\"\n",
        "    s = _hyphen_re.sub(\" \", s)\n",
        "    # optionally remove isolated izāfa tokens like 'e' or 'i'\n",
        "    if REMOVE_IZAF:\n",
        "        s = re.sub(r'\\b(e|i)\\b', ' ', s)\n",
        "    # collapse whitespace\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "# ---- BPE Core ----\n",
        "SPACE = \"▁\"\n",
        "SPECIALS = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\n",
        "PAD,BOS,EOS,UNK = range(4)\n",
        "\n",
        "def _words(lines, is_urdu=False):\n",
        "    \"\"\"\n",
        "    Yield token sequences for BPE learning.\n",
        "    Each yielded seq is: [SPACE] + list(characters of token)\n",
        "    Tokens are split on spaces or hyphens (consistent across pipeline).\n",
        "    \"\"\"\n",
        "    for ln in lines:\n",
        "        ln = normalize_urdu(ln) if is_urdu else normalize_roman(ln)\n",
        "        if not ln:\n",
        "            continue\n",
        "        # split on whitespace or hyphen variants\n",
        "        for w in _split_on_hyphen_or_space.split(ln):\n",
        "            if not w:\n",
        "                continue\n",
        "            yield [SPACE] + list(w)\n",
        "\n",
        "def _count_pairs(seqs):\n",
        "    pairs = Counter()\n",
        "    for seq in seqs:\n",
        "        for i in range(len(seq)-1):\n",
        "            pairs[(seq[i], seq[i+1])] += 1\n",
        "    return pairs\n",
        "\n",
        "def _merge_seq(seq, pair):\n",
        "    a,b = pair\n",
        "    out=[]; i=0\n",
        "    while i < len(seq):\n",
        "        if i+1 < len(seq) and seq[i]==a and seq[i+1]==b:\n",
        "            out.append(a+b); i+=2\n",
        "        else:\n",
        "            out.append(seq[i]); i+=1\n",
        "    return out\n",
        "#voc size 8000\n",
        "def learn_bpe(train_lines, vocab_size=200, min_count=2, is_urdu=False, verbose_every=100):\n",
        "    seqs = list(_words(train_lines, is_urdu=is_urdu))\n",
        "    symbols = Counter()\n",
        "    for s in seqs: symbols.update(s)\n",
        "    itos = SPECIALS + sorted(symbols.keys())\n",
        "    stoi = {t:i for i,t in enumerate(itos)}\n",
        "    merges = []\n",
        "    step=0\n",
        "    while len(itos) < vocab_size:\n",
        "        pairs = _count_pairs(seqs)\n",
        "        if not pairs: break\n",
        "        (a,b), c = pairs.most_common(1)[0]\n",
        "        if c < min_count: break\n",
        "        seqs = [_merge_seq(s, (a,b)) for s in seqs]\n",
        "        tok = a+b\n",
        "        if tok not in stoi:\n",
        "            stoi[tok] = len(itos); itos.append(tok)\n",
        "        merges.append((a,b))\n",
        "        step+=1\n",
        "        if verbose_every and step%verbose_every==0:\n",
        "            print(f\"[BPE] step={step} merge=({a},{b}) freq={c} vocab={len(itos)}\")\n",
        "    print(f\"[BPE] Done. merges={len(merges)} vocab={len(itos)}\")\n",
        "    return {\"itos\": itos, \"merges\": merges, \"vocab_size\": len(itos)}\n",
        "\n",
        "def apply_bpe(line, model, add_bos_eos=True, is_urdu=False, debug=False):\n",
        "    merges = model[\"merges\"]\n",
        "    itos = model[\"itos\"]\n",
        "    stoi = {t:i for i,t in enumerate(itos)}\n",
        "    line = normalize_urdu(line) if is_urdu else normalize_roman(line)\n",
        "    # split tokens consistently\n",
        "    words = [] if not line else [w for w in _split_on_hyphen_or_space.split(line) if w]\n",
        "    pieces=[]\n",
        "    if words:\n",
        "        for w in words:\n",
        "            seq = [SPACE] + list(w)\n",
        "            for a,b in merges:\n",
        "                seq = _merge_seq(seq, (a,b))\n",
        "            if debug: print(f\"[ENC] word={w} -> pieces={seq}\")\n",
        "            pieces.extend(seq)\n",
        "    ids = [stoi.get(p, UNK) for p in pieces]\n",
        "    if add_bos_eos: ids = [BOS] + ids + [EOS]\n",
        "    if debug: print(f\"[ENC] ids={ids}\")\n",
        "    return ids\n",
        "\n",
        "def save_bpe(model, path):\n",
        "    with open(path,\"w\",encoding=\"utf-8\") as f:\n",
        "        json.dump(model,f,ensure_ascii=False)\n",
        "def load_bpe(path):\n",
        "    with open(path,encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# ---- Preview helper ----\n",
        "def preview_word_pairs(u_line, r_line, max_pairs=10):\n",
        "    u_line_n = normalize_urdu(u_line)\n",
        "    r_line_n = normalize_roman(r_line)\n",
        "    u_words = [w for w in _split_on_hyphen_or_space.split(u_line_n) if w]\n",
        "    r_words = [w for w in _split_on_hyphen_or_space.split(r_line_n) if w]\n",
        "    if not u_words or not r_words:\n",
        "        return\n",
        "    if abs(len(u_words)-len(r_words))>2:\n",
        "        return\n",
        "    m = min(len(u_words), len(r_words), max_pairs)\n",
        "    print(\"[PAIR preview]\")\n",
        "    for i in range(m):\n",
        "        print(f'  \"{u_words[i]}\" -> {r_words[i]}')\n",
        "\n",
        "# ---- Collect Urdu–Roman line pairs ----\n",
        "pairs = []\n",
        "poets = sorted([p for p in RAW_ROOT.iterdir() if p.is_dir()])\n",
        "print(\"Poets found:\", len(poets))\n",
        "\n",
        "for poet in poets:\n",
        "    ur_dir = poet/\"ur\"\n",
        "    en_dir = poet/\"en\"\n",
        "    if not ur_dir.exists() or not en_dir.exists():\n",
        "        continue\n",
        "    ur_files = {p.name:p for p in ur_dir.iterdir() if p.is_file()}\n",
        "    en_files = {p.name:p for p in en_dir.iterdir() if p.is_file()}\n",
        "    shared = sorted(set(ur_files) & set(en_files))\n",
        "    for name in shared:\n",
        "        ur_text = ur_files[name].read_text(encoding=\"utf-8\",errors=\"ignore\")\n",
        "        en_text = en_files[name].read_text(encoding=\"utf-8\",errors=\"ignore\")\n",
        "        ur_lines = [normalize_urdu(x) for x in ur_text.splitlines() if x.strip()]\n",
        "        en_lines = [normalize_roman(x) for x in en_text.splitlines() if x.strip()]\n",
        "        m = min(len(ur_lines), len(en_lines))\n",
        "        for i in range(m):\n",
        "            u, r = ur_lines[i], en_lines[i]\n",
        "            if 2 <= len(u.split()) <= 60 and 1 <= len(r.split()) <= 70:\n",
        "                pairs.append((u,r))\n",
        "                # preview some random pairs\n",
        "                if random.random() < 0.0005:\n",
        "                    preview_word_pairs(u,r)\n",
        "\n",
        "print(\"Total line pairs collected:\", len(pairs))\n",
        "assert pairs, \"No pairs collected!\"\n",
        "\n",
        "# ---- Train/Val/Test Split ----\n",
        "random.seed(42); random.shuffle(pairs)\n",
        "n=len(pairs); n_train=int(0.5*n); n_val=int(0.25*n)\n",
        "train=pairs[:n_train]; val=pairs[n_train:n_train+n_val]; test=pairs[n_train+n_val:]\n",
        "\n",
        "def dump(split,name):\n",
        "    (OUT/f\"{name}.src\").write_text(\"\\n\".join(u for u,_ in split)+\"\\n\", encoding=\"utf-8\")\n",
        "    (OUT/f\"{name}.tgt\").write_text(\"\\n\".join(r for _,r in split)+\"\\n\", encoding=\"utf-8\")\n",
        "\n",
        "dump(train,\"train\"); dump(val,\"val\"); dump(test,\"test\")\n",
        "print(\"Saved splits:\", OUT, \"sizes:\", len(train), len(val), len(test))\n",
        "\n",
        "# ---- Optional: train BPE on both sides ----\n",
        "urdu_lines=[u for u,_ in train]\n",
        "roman_lines=[r for _,r in train]\n",
        "#voc size 2000\n",
        "ur_model=learn_bpe(urdu_lines, vocab_size=200, is_urdu=True, verbose_every=200)\n",
        "ro_model=learn_bpe(roman_lines, vocab_size=200, is_urdu=False, verbose_every=200)\n",
        "save_bpe(ur_model, OUT/\"bpe_ur.json\")\n",
        "save_bpe(ro_model, OUT/\"bpe_ro.json\")\n",
        "\n",
        "# ---- Quick Debug ----\n",
        "sample_u,sample_r = pairs[0]\n",
        "print(\"\\nSample Encoding:\")\n",
        "print(\"Urdu:\", sample_u)\n",
        "apply_bpe(sample_u, ur_model, is_urdu=True, debug=True)\n",
        "print(\"Roman:\", sample_r)\n",
        "apply_bpe(sample_r, ro_model, is_urdu=False, debug=True)\n",
        "preview_word_pairs(sample_u,sample_r)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2jCCipt9Nwy",
        "outputId": "d9d51d11-ff42-47f2-e3a1-d6b465b2c985"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 126 53 27 7 154 78 68 64 33 13 27 13 96 74 32 13 106 81 67 183 60 2\n",
            "1 70 13 96 61 32 48 84 137 67 14 81 64 115 54 76 18 35 75 69 68 2\n",
            "1 59 22 20 121 105 13 17 23 72 64 36 59 36 63 60 86 54 135 25 75 35 20 62 13 110 102 2\n",
            "-----\n",
            "1 171 13 8 16 33 5 79 21 71 52 49 8 24 35 33 35 50 75 8 13 35 108 74 57 114 15 61 2\n",
            "1 60 35 50 54 13 14 90 183 57 87 74 160 17 113 137 20 92 59 52 2\n",
            "1 47 48 11 138 92 17 16 33 88 49 123 77 21 46 16 146 151 12 172 97 122 20 186 51 35 29 48 105 2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ---- Encode splits into ID files ----\n",
        "DATA = OUT  # same as OUT\n",
        "src_model = load_bpe(DATA/\"bpe_ur.json\")   # Urdu model\n",
        "tgt_model = load_bpe(DATA/\"bpe_ro.json\")   # Roman model\n",
        "\n",
        "def encode_file_to_ids(in_path, out_path, model, is_urdu=False):\n",
        "    lines = Path(in_path).read_text(encoding=\"utf-8\").splitlines()\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for ln in lines:\n",
        "            ids = apply_bpe(ln, model, add_bos_eos=True, is_urdu=is_urdu)\n",
        "            f.write(\" \".join(map(str, ids)) + \"\\n\")\n",
        "\n",
        "# Urdu source side\n",
        "encode_file_to_ids(DATA/\"train.src\", DATA/\"train.src.ids\", src_model, is_urdu=True)\n",
        "encode_file_to_ids(DATA/\"val.src\",   DATA/\"val.src.ids\",   src_model, is_urdu=True)\n",
        "encode_file_to_ids(DATA/\"test.src\",  DATA/\"test.src.ids\",  src_model, is_urdu=True)\n",
        "\n",
        "# Roman target side\n",
        "encode_file_to_ids(DATA/\"train.tgt\", DATA/\"train.tgt.ids\", tgt_model, is_urdu=False)\n",
        "encode_file_to_ids(DATA/\"val.tgt\",   DATA/\"val.tgt.ids\",   tgt_model, is_urdu=False)\n",
        "encode_file_to_ids(DATA/\"test.tgt\",  DATA/\"test.tgt.ids\",  tgt_model, is_urdu=False)\n",
        "\n",
        "# Quick check\n",
        "!sed -n '1,3p' \"{DATA}/train.src.ids\"; echo \"-----\"; sed -n '1,3p' \"{DATA}/train.tgt.ids\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-GDO89UFHTx",
        "outputId": "09b4b2bf-beb9-40ea-9e37-62c69ed99c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK → logits shape: (4, 18, 200)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# ===== BiLSTM Encoder (2 layers) + LSTM Decoder (4 layers) =====\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# --- specials (same as preprocessing) ---\n",
        "PAD, BOS, EOS, UNK = 0, 1, 2, 3\n",
        "\n",
        "# --- Encoder: 2-layer BiLSTM ---\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=256, hid=512, n_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
        "        # BiLSTM -> hidden per direction = hid//2, concat => hid\n",
        "        self.rnn = nn.LSTM(\n",
        "            emb_dim, hid // 2, num_layers=n_layers, batch_first=True,\n",
        "            dropout=dropout if n_layers > 1 else 0.0, bidirectional=True\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.emb(x)                  # (B, T, E)\n",
        "        out, (h, c) = self.rnn(emb)       # out: (B, T, H)  h/c: (2*n_layers, B, H/2)\n",
        "        return out, (h, c)\n",
        "\n",
        "# --- Decoder: 4-layer LSTM ---\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=256, hid=512, n_layers=4, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
        "        self.rnn = nn.LSTM(\n",
        "            emb_dim, hid, num_layers=n_layers, batch_first=True,\n",
        "            dropout=dropout if n_layers > 1 else 0.0\n",
        "        )\n",
        "        self.fc = nn.Linear(hid, vocab_size)\n",
        "\n",
        "    def forward(self, y_in, state):\n",
        "        emb = self.emb(y_in)               # (B, T_in, E)\n",
        "        out, state = self.rnn(emb, state)  # (B, T_in, H)\n",
        "        logits = self.fc(out)              # (B, T_in, V)\n",
        "        return logits, state\n",
        "\n",
        "# --- Seq2Seq wrapper + bridge (encoder -> decoder states) ---\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, src_vocab, tgt_vocab, emb=256, hid=512, enc_layers=2, dec_layers=4, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, emb, hid, enc_layers, dropout)\n",
        "        self.decoder = Decoder(tgt_vocab, emb, hid, dec_layers, dropout)\n",
        "        self.bridge_h = nn.Linear(hid, hid)\n",
        "        self.bridge_c = nn.Linear(hid, hid)\n",
        "\n",
        "    def init_dec_state(self, h, c):\n",
        "        # take last forward/backward enc states and concat\n",
        "        H_enc = torch.cat([h[-2], h[-1]], dim=-1)  # (B, H)\n",
        "        C_enc = torch.cat([c[-2], c[-1]], dim=-1)  # (B, H)\n",
        "        dec_h0 = self.bridge_h(H_enc).unsqueeze(0).repeat(self.decoder.rnn.num_layers, 1, 1)\n",
        "        dec_c0 = self.bridge_c(C_enc).unsqueeze(0).repeat(self.decoder.rnn.num_layers, 1, 1)\n",
        "        return dec_h0, dec_c0\n",
        "\n",
        "    def forward(self, src, tgt_in):\n",
        "        enc_out, (h, c) = self.encoder(src)\n",
        "        dec_state = self.init_dec_state(h, c)\n",
        "        logits, _ = self.decoder(tgt_in, dec_state)   # (B, T_in, V)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ===== Instantiate with your actual vocab sizes =====\n",
        "PROJECT = Path(\"/content/drive/MyDrive/25F-7801\")\n",
        "BPE_DIR = PROJECT / \"data/processed\"\n",
        "\n",
        "src_vocab = len(json.load(open(BPE_DIR/\"bpe_ur.json\", encoding=\"utf-8\"))[\"itos\"])\n",
        "tgt_vocab = len(json.load(open(BPE_DIR/\"bpe_ro.json\", encoding=\"utf-8\"))[\"itos\"])\n",
        "\n",
        "model = Seq2Seq(\n",
        "    src_vocab=src_vocab, tgt_vocab=tgt_vocab,\n",
        "    emb=256, hid=512, enc_layers=2, dec_layers=4, dropout=0.3\n",
        ")\n",
        "\n",
        "# quick shape test\n",
        "B, Tsrc, Ttgt = 4, 20, 18\n",
        "x = torch.randint(low=0, high=src_vocab, size=(B, Tsrc))\n",
        "y_in = torch.randint(low=0, high=tgt_vocab, size=(B, Ttgt))\n",
        "with torch.no_grad():\n",
        "    out = model(x, y_in)\n",
        "print(\"OK → logits shape:\", tuple(out.shape))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yD8ZvxPEHYjv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbfbe498-d54b-4151-eb98-884687d0c78c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src batch: torch.Size([64, 30])\n",
            "tgt batch: torch.Size([64, 33])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# ===== Dataset + Dataloader for NMT =====\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "\n",
        "PAD, BOS, EOS, UNK = 0, 1, 2, 3  # same specials as preprocessing + model\n",
        "\n",
        "# --- helper to read .ids files (already BPE-encoded) ---\n",
        "def read_ids(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return [list(map(int, ln.strip().split())) for ln in f if ln.strip()]\n",
        "\n",
        "# --- Dataset class ---\n",
        "class NMTDataset(Dataset):\n",
        "    def __init__(self, src_ids_path, tgt_ids_path, max_len=128):\n",
        "        self.src = read_ids(src_ids_path)\n",
        "        self.tgt = read_ids(tgt_ids_path)\n",
        "        assert len(self.src) == len(self.tgt), \"Mismatch between src and tgt lines!\"\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.src[idx][:self.max_len]\n",
        "        t = self.tgt[idx][:self.max_len]\n",
        "        return torch.tensor(s, dtype=torch.long), torch.tensor(t, dtype=torch.long)\n",
        "\n",
        "# --- Collate fn (for padding in batch) ---\n",
        "def collate(batch):\n",
        "    srcs, tgts = zip(*batch)\n",
        "    max_s = max(len(x) for x in srcs)\n",
        "    max_t = max(len(x) for x in tgts)\n",
        "    src_pad = torch.full((len(batch), max_s), PAD, dtype=torch.long)\n",
        "    tgt_pad = torch.full((len(batch), max_t), PAD, dtype=torch.long)\n",
        "    for i, (s, t) in enumerate(zip(srcs, tgts)):\n",
        "        src_pad[i, :len(s)] = s\n",
        "        tgt_pad[i, :len(t)] = t\n",
        "    return src_pad, tgt_pad\n",
        "\n",
        "# ===== Load train/val/test sets =====\n",
        "PROJECT = Path(\"/content/drive/MyDrive/25F-7801\")\n",
        "DATA = PROJECT / \"data/processed\"\n",
        "\n",
        "train_set = NMTDataset(DATA/\"train.src.ids\", DATA/\"train.tgt.ids\")\n",
        "val_set   = NMTDataset(DATA/\"val.src.ids\", DATA/\"val.tgt.ids\")\n",
        "test_set  = NMTDataset(DATA/\"test.src.ids\", DATA/\"test.tgt.ids\")\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, collate_fn=collate)\n",
        "val_loader   = DataLoader(val_set, batch_size=64, shuffle=False, collate_fn=collate)\n",
        "test_loader  = DataLoader(test_set, batch_size=64, shuffle=False, collate_fn=collate)\n",
        "\n",
        "# quick check\n",
        "for src, tgt in train_loader:\n",
        "    print(\"src batch:\", src.shape)\n",
        "    print(\"tgt batch:\", tgt.shape)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn, math, json\n",
        "from pathlib import Path\n",
        "\n",
        "# === Vocab Sizes ===\n",
        "with open(DATA / \"bpe_ur.json\", encoding=\"utf-8\") as f:\n",
        "    src_vocab = len(json.load(f)[\"itos\"])\n",
        "with open(DATA / \"bpe_ro.json\", encoding=\"utf-8\") as f:\n",
        "    tgt_vocab = len(json.load(f)[\"itos\"])\n",
        "\n",
        "# === Device ===\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# === Model ===\n",
        "model = model.to(device)\n",
        "\n",
        "# === Loss & Optimizer ===\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "\n",
        "\n",
        "\n",
        "# === Training / Validation Loop ===\n",
        "def run_epoch(loader, train=True):\n",
        "    model.train(mode=train)\n",
        "    total, steps = 0.0, 0\n",
        "\n",
        "    for src, tgt in loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        # Teacher forcing\n",
        "        inp  = tgt[:, :-1]  # decoder input\n",
        "        gold = tgt[:, 1:].contiguous().view(-1)  # expected output\n",
        "\n",
        "        logits = model(src, inp).contiguous().view(-1, tgt_vocab)\n",
        "        loss = criterion(logits, gold)\n",
        "\n",
        "        if train:\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optim.step()\n",
        "\n",
        "        total += loss.item()\n",
        "        steps += 1\n",
        "\n",
        "    return total / max(steps, 1)\n",
        "\n",
        "# === Checkpoint Saving ===\n",
        "best_path = PROJECT / \"checkpoints\" / \"best.pt\"\n",
        "best_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "EPOCHS = 15\n",
        "\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    tr = run_epoch(train_loader, train=True)\n",
        "    vl = run_epoch(val_loader,   train=False)\n",
        "    ppl = math.exp(min(vl, 20))  # clamp to avoid overflow\n",
        "\n",
        "    print(f\"Epoch {ep:02d} | train_loss {tr:.3f} | val_loss {vl:.3f} | val_ppl {ppl:.2f}\")\n",
        "\n",
        "    if vl < best_val:\n",
        "        best_val = vl\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model\": model.state_dict(),\n",
        "                \"src_vocab\": src_vocab,\n",
        "                \"tgt_vocab\": tgt_vocab,\n",
        "            },\n",
        "            best_path,\n",
        "        )\n",
        "        print(\"  ↳ saved:\", best_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EQQcWS2QgwC",
        "outputId": "8b6ca3af-cfcb-4490-8563-9d31fb1bc475"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch 01 | train_loss 0.406 | val_loss 1.277 | val_ppl 3.59\n",
            "  ↳ saved: /content/drive/MyDrive/25F-7801/checkpoints/best.pt\n",
            "Epoch 02 | train_loss 0.365 | val_loss 1.290 | val_ppl 3.63\n",
            "Epoch 03 | train_loss 0.333 | val_loss 1.294 | val_ppl 3.65\n",
            "Epoch 04 | train_loss 0.307 | val_loss 1.311 | val_ppl 3.71\n",
            "Epoch 05 | train_loss 0.285 | val_loss 1.313 | val_ppl 3.72\n",
            "Epoch 06 | train_loss 0.263 | val_loss 1.323 | val_ppl 3.75\n",
            "Epoch 07 | train_loss 0.245 | val_loss 1.315 | val_ppl 3.72\n",
            "Epoch 08 | train_loss 0.226 | val_loss 1.337 | val_ppl 3.81\n",
            "Epoch 09 | train_loss 0.209 | val_loss 1.351 | val_ppl 3.86\n",
            "Epoch 10 | train_loss 0.199 | val_loss 1.351 | val_ppl 3.86\n",
            "Epoch 11 | train_loss 0.182 | val_loss 1.369 | val_ppl 3.93\n",
            "Epoch 12 | train_loss 0.170 | val_loss 1.368 | val_ppl 3.93\n",
            "Epoch 13 | train_loss 0.159 | val_loss 1.369 | val_ppl 3.93\n",
            "Epoch 14 | train_loss 0.147 | val_loss 1.370 | val_ppl 3.94\n",
            "Epoch 15 | train_loss 0.144 | val_loss 1.383 | val_ppl 3.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== RUN EXPERIMENTS (no evaluation) ======\n",
        "import math, time, json, torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# --- paths ---\n",
        "PROJECT  = Path(\"/content/drive/MyDrive/25F-7801\")\n",
        "DATA     = PROJECT/\"data/processed\"\n",
        "CKPT_DIR = PROJECT/\"checkpoints\"; CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "RES_DIR  = PROJECT/\"results\";     RES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- quick checks ---\n",
        "assert \"Seq2Seq\" in globals(), \"Seq2Seq not defined — re-run your model-architecture cell.\"\n",
        "for p in [\"train.src.ids\",\"train.tgt.ids\",\"val.src.ids\",\"val.tgt.ids\"]:\n",
        "    assert (DATA/p).exists(), f\"Missing {p} — re-run BPE encode step.\"\n",
        "\n",
        "PAD, BOS, EOS, UNK = 0, 1, 2, 3\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# --- vocab sizes ---\n",
        "src_vocab = len(json.load(open(DATA/\"bpe_ur.json\", encoding=\"utf-8\"))[\"itos\"])\n",
        "tgt_vocab = len(json.load(open(DATA/\"bpe_ro.json\", encoding=\"utf-8\"))[\"itos\"])\n",
        "\n",
        "# --- dataset/dataloader helpers ---\n",
        "def read_ids(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return [list(map(int, ln.strip().split())) for ln in f if ln.strip()]\n",
        "\n",
        "class NMTDataset(Dataset):\n",
        "    def __init__(self, src_path, tgt_path, max_len=128):\n",
        "        self.src = read_ids(src_path); self.tgt = read_ids(tgt_path)\n",
        "        assert len(self.src)==len(self.tgt)\n",
        "        self.max_len = max_len\n",
        "    def __len__(self): return len(self.src)\n",
        "    def __getitem__(self, i):\n",
        "        s = self.src[i][:self.max_len]; t = self.tgt[i][:self.max_len]\n",
        "        return torch.tensor(s), torch.tensor(t)\n",
        "\n",
        "def collate(batch):\n",
        "    srcs, tgts = zip(*batch)\n",
        "    max_s=max(len(x) for x in srcs); max_t=max(len(x) for x in tgts)\n",
        "    S=torch.full((len(batch),max_s), PAD, dtype=torch.long)\n",
        "    T=torch.full((len(batch),max_t), PAD, dtype=torch.long)\n",
        "    for i,(s,t) in enumerate(zip(srcs,tgts)):\n",
        "        S[i,:len(s)]=s; T[i,:len(t)]=t\n",
        "    return S,T\n",
        "\n",
        "# --- one experiment: train & return metrics ---\n",
        "def run_experiment(tag, emb, hid, enc_layers, dec_layers, dropout, lr, batch, epochs=8, max_len=128):\n",
        "    train_ds=NMTDataset(DATA/\"train.src.ids\", DATA/\"train.tgt.ids\", max_len=max_len)\n",
        "    val_ds  =NMTDataset(DATA/\"val.src.ids\",   DATA/\"val.tgt.ids\",   max_len=max_len)\n",
        "    train_ld=DataLoader(train_ds, batch_size=batch, shuffle=True,  collate_fn=collate)\n",
        "    val_ld  =DataLoader(val_ds,   batch_size=batch, shuffle=False, collate_fn=collate)\n",
        "\n",
        "    model = Seq2Seq(src_vocab, tgt_vocab, emb=emb, hid=hid,\n",
        "                    enc_layers=enc_layers, dec_layers=dec_layers, dropout=dropout).to(device)\n",
        "    crit  = nn.CrossEntropyLoss(ignore_index=PAD)\n",
        "    optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    def run_epoch(loader, train=True):\n",
        "        model.train(mode=train); total=0.0; steps=0\n",
        "        for src, tgt in loader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            inp  = tgt[:, :-1]\n",
        "            gold = tgt[:, 1:].contiguous().view(-1)\n",
        "            logits = model(src, inp).contiguous().view(-1, tgt_vocab)\n",
        "            loss = crit(logits, gold)\n",
        "            if train:\n",
        "                optim.zero_grad(); loss.backward()\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optim.step()\n",
        "            total += float(loss); steps += 1\n",
        "        return total/max(steps,1)\n",
        "\n",
        "    best_val=float(\"inf\"); t0=time.time()\n",
        "    for ep in range(1, epochs+1):\n",
        "        tr = run_epoch(train_ld, True)\n",
        "        vl = run_epoch(val_ld,   False)\n",
        "        ppl = math.exp(min(vl, 20))\n",
        "        print(f\"[{tag}] Ep {ep:02d} | train {tr:.3f} | val {vl:.3f} | ppl {ppl:.2f}\")\n",
        "        if vl < best_val:\n",
        "            best_val = vl\n",
        "            torch.save({\"model\": model.state_dict(),\n",
        "                        \"src_vocab\": src_vocab,\n",
        "                        \"tgt_vocab\": tgt_vocab}, CKPT_DIR/f\"best_{tag}.pt\")\n",
        "\n",
        "    dur = round(time.time()-t0,1)\n",
        "    return dict(tag=tag, emb=emb, hid=hid, enc_layers=enc_layers, dec_layers=dec_layers,\n",
        "                dropout=dropout, lr=lr, batch=batch, epochs=epochs, max_len=max_len,\n",
        "                val_loss=round(best_val,4), val_ppl=round(math.exp(min(best_val,20)),2),\n",
        "                time_s=dur, ckpt=str(CKPT_DIR/f\"best_{tag}.pt\"))\n",
        "\n",
        "# --- define experiments ---\n",
        "configs = [\n",
        "    dict(tag=\"A_emb128_hid256_do0.1\", emb=128, hid=256, enc_layers=2, dec_layers=4, dropout=0.1, lr=1e-3, batch=32, epochs=10),\n",
        "    dict(tag=\"B_emb256_hid512_do0.3\", emb=256, hid=512, enc_layers=2, dec_layers=4, dropout=0.3, lr=5e-4, batch=64, epochs=10),\n",
        "    dict(tag=\"C_emb512_hid512_do0.5\", emb=512, hid=512, enc_layers=2, dec_layers=4, dropout=0.5, lr=1e-4, batch=64, epochs=10),\n",
        "]\n",
        "\n",
        "# --- run all and save CSV ---\n",
        "rows=[]\n",
        "for cfg in configs:\n",
        "    print(\"\\n====== Running:\", cfg[\"tag\"], \"======\")\n",
        "    rows.append(run_experiment(**cfg))\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "display(df)\n",
        "out_csv = RES_DIR/\"experiments_val_only.csv\"\n",
        "df.to_csv(out_csv, index=False)\n",
        "print(\"Saved:\", out_csv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 855
        },
        "id": "ULjUULl5S_Tr",
        "outputId": "0643cfd1-c4b0-4b58-bf84-d4f5ed34835d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "\n",
            "====== Running: A_emb128_hid256_do0.1 ======\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3213803873.py:75: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n",
            "  total += float(loss); steps += 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[A_emb128_hid256_do0.1] Ep 01 | train 4.762 | val 4.440 | ppl 84.73\n",
            "[A_emb128_hid256_do0.1] Ep 02 | train 4.042 | val 3.735 | ppl 41.90\n",
            "[A_emb128_hid256_do0.1] Ep 03 | train 3.523 | val 3.332 | ppl 27.99\n",
            "[A_emb128_hid256_do0.1] Ep 04 | train 3.174 | val 3.045 | ppl 21.00\n",
            "[A_emb128_hid256_do0.1] Ep 05 | train 2.893 | val 2.816 | ppl 16.70\n",
            "[A_emb128_hid256_do0.1] Ep 06 | train 2.669 | val 2.659 | ppl 14.28\n",
            "[A_emb128_hid256_do0.1] Ep 07 | train 2.488 | val 2.514 | ppl 12.35\n",
            "[A_emb128_hid256_do0.1] Ep 08 | train 2.335 | val 2.413 | ppl 11.17\n",
            "[A_emb128_hid256_do0.1] Ep 09 | train 2.197 | val 2.329 | ppl 10.27\n",
            "[A_emb128_hid256_do0.1] Ep 10 | train 2.071 | val 2.245 | ppl 9.44\n",
            "\n",
            "====== Running: B_emb256_hid512_do0.3 ======\n",
            "[B_emb256_hid512_do0.3] Ep 01 | train 4.706 | val 4.488 | ppl 88.91\n",
            "[B_emb256_hid512_do0.3] Ep 02 | train 4.064 | val 3.608 | ppl 36.89\n",
            "[B_emb256_hid512_do0.3] Ep 03 | train 3.252 | val 3.082 | ppl 21.81\n",
            "[B_emb256_hid512_do0.3] Ep 04 | train 2.823 | val 2.693 | ppl 14.78\n",
            "[B_emb256_hid512_do0.3] Ep 05 | train 2.439 | val 2.426 | ppl 11.31\n",
            "[B_emb256_hid512_do0.3] Ep 06 | train 2.153 | val 2.201 | ppl 9.03\n",
            "[B_emb256_hid512_do0.3] Ep 07 | train 1.912 | val 2.033 | ppl 7.64\n",
            "[B_emb256_hid512_do0.3] Ep 08 | train 1.696 | val 1.865 | ppl 6.46\n",
            "[B_emb256_hid512_do0.3] Ep 09 | train 1.512 | val 1.759 | ppl 5.81\n",
            "[B_emb256_hid512_do0.3] Ep 10 | train 1.355 | val 1.666 | ppl 5.29\n",
            "\n",
            "====== Running: C_emb512_hid512_do0.5 ======\n",
            "[C_emb512_hid512_do0.5] Ep 01 | train 4.899 | val 4.782 | ppl 119.30\n",
            "[C_emb512_hid512_do0.5] Ep 02 | train 4.699 | val 4.614 | ppl 100.85\n",
            "[C_emb512_hid512_do0.5] Ep 03 | train 4.516 | val 4.415 | ppl 82.71\n",
            "[C_emb512_hid512_do0.5] Ep 04 | train 4.327 | val 4.264 | ppl 71.07\n",
            "[C_emb512_hid512_do0.5] Ep 05 | train 4.172 | val 4.135 | ppl 62.51\n",
            "[C_emb512_hid512_do0.5] Ep 06 | train 4.051 | val 4.108 | ppl 60.84\n",
            "[C_emb512_hid512_do0.5] Ep 07 | train 3.940 | val 3.933 | ppl 51.08\n",
            "[C_emb512_hid512_do0.5] Ep 08 | train 3.827 | val 3.774 | ppl 43.55\n",
            "[C_emb512_hid512_do0.5] Ep 09 | train 3.628 | val 3.531 | ppl 34.15\n",
            "[C_emb512_hid512_do0.5] Ep 10 | train 3.391 | val 3.392 | ppl 29.71\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                     tag  emb  hid  enc_layers  dec_layers  dropout      lr  \\\n",
              "0  A_emb128_hid256_do0.1  128  256           2           4      0.1  0.0010   \n",
              "1  B_emb256_hid512_do0.3  256  512           2           4      0.3  0.0005   \n",
              "2  C_emb512_hid512_do0.5  512  512           2           4      0.5  0.0001   \n",
              "\n",
              "   batch  epochs  max_len  val_loss  val_ppl  time_s  \\\n",
              "0     32      10      128    2.2446     9.44    54.4   \n",
              "1     64      10      128    1.6657     5.29   102.3   \n",
              "2     64      10      128    3.3916    29.71   110.9   \n",
              "\n",
              "                                                ckpt  \n",
              "0  /content/drive/MyDrive/25F-7801/checkpoints/be...  \n",
              "1  /content/drive/MyDrive/25F-7801/checkpoints/be...  \n",
              "2  /content/drive/MyDrive/25F-7801/checkpoints/be...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0e494d68-8e2e-4b78-a945-d9e5126e187e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tag</th>\n",
              "      <th>emb</th>\n",
              "      <th>hid</th>\n",
              "      <th>enc_layers</th>\n",
              "      <th>dec_layers</th>\n",
              "      <th>dropout</th>\n",
              "      <th>lr</th>\n",
              "      <th>batch</th>\n",
              "      <th>epochs</th>\n",
              "      <th>max_len</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_ppl</th>\n",
              "      <th>time_s</th>\n",
              "      <th>ckpt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A_emb128_hid256_do0.1</td>\n",
              "      <td>128</td>\n",
              "      <td>256</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>10</td>\n",
              "      <td>128</td>\n",
              "      <td>2.2446</td>\n",
              "      <td>9.44</td>\n",
              "      <td>54.4</td>\n",
              "      <td>/content/drive/MyDrive/25F-7801/checkpoints/be...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>B_emb256_hid512_do0.3</td>\n",
              "      <td>256</td>\n",
              "      <td>512</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>64</td>\n",
              "      <td>10</td>\n",
              "      <td>128</td>\n",
              "      <td>1.6657</td>\n",
              "      <td>5.29</td>\n",
              "      <td>102.3</td>\n",
              "      <td>/content/drive/MyDrive/25F-7801/checkpoints/be...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>C_emb512_hid512_do0.5</td>\n",
              "      <td>512</td>\n",
              "      <td>512</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>64</td>\n",
              "      <td>10</td>\n",
              "      <td>128</td>\n",
              "      <td>3.3916</td>\n",
              "      <td>29.71</td>\n",
              "      <td>110.9</td>\n",
              "      <td>/content/drive/MyDrive/25F-7801/checkpoints/be...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e494d68-8e2e-4b78-a945-d9e5126e187e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0e494d68-8e2e-4b78-a945-d9e5126e187e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0e494d68-8e2e-4b78-a945-d9e5126e187e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-560213ad-ecb3-4339-9e73-48fa935d8f7a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-560213ad-ecb3-4339-9e73-48fa935d8f7a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-560213ad-ecb3-4339-9e73-48fa935d8f7a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_26c2e3da-ff9b-4798-9e9e-679807083b07\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_26c2e3da-ff9b-4798-9e9e-679807083b07 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"tag\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"A_emb128_hid256_do0.1\",\n          \"B_emb256_hid512_do0.3\",\n          \"C_emb512_hid512_do0.5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"emb\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 195,\n        \"min\": 128,\n        \"max\": 512,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          128,\n          256,\n          512\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 147,\n        \"min\": 256,\n        \"max\": 512,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          512,\n          256\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"enc_layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dec_layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2,\n        \"min\": 0.1,\n        \"max\": 0.5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0004509249752822894,\n        \"min\": 0.0001,\n        \"max\": 0.001,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18,\n        \"min\": 32,\n        \"max\": 64,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"epochs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 10,\n        \"max\": 10,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"max_len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 128,\n        \"max\": 128,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8783948447784363,\n        \"min\": 1.6657,\n        \"max\": 3.3916,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.2446\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_ppl\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13.066699404720895,\n        \"min\": 5.29,\n        \"max\": 29.71,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          9.44\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time_s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30.44289736539543,\n        \"min\": 54.4,\n        \"max\": 110.9,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          54.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ckpt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"/content/drive/MyDrive/25F-7801/checkpoints/best_A_emb128_hid256_do0.1.pt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/25F-7801/results/experiments_val_only.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install sacrebleu python-Levenshtein\n",
        "import torch, json, math, sacrebleu, Levenshtein\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "# --- paths ---\n",
        "PROJECT = Path(\"/content/drive/MyDrive/25F-7801\")\n",
        "DATA    = PROJECT/\"data/processed\"\n",
        "CKPT    = PROJECT/\"checkpoints/best.pt\"\n",
        "\n",
        "# --- specials ---\n",
        "PAD, BOS, EOS, UNK = 0,1,2,3\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# --- model class must exist ---\n",
        "assert \"Seq2Seq\" in globals(), \"Seq2Seq class missing — re-run the model-architecture cell.\"\n",
        "\n",
        "# --- vocab sizes (directly from processed folder) ---\n",
        "src_vocab = len(json.load(open(DATA/\"bpe_ur.json\", encoding=\"utf-8\"))[\"itos\"])\n",
        "tgt_vocab = len(json.load(open(DATA/\"bpe_ro.json\", encoding=\"utf-8\"))[\"itos\"])\n",
        "\n",
        "# --- dataset & collate ---\n",
        "def read_ids(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return [list(map(int, ln.strip().split())) for ln in f if ln.strip()]\n",
        "\n",
        "class NMTDataset(Dataset):\n",
        "    def __init__(self, src_path, tgt_path, max_len=128):\n",
        "        self.src = read_ids(src_path); self.tgt = read_ids(tgt_path)\n",
        "        assert len(self.src)==len(self.tgt)\n",
        "        self.max_len = max_len\n",
        "    def __len__(self): return len(self.src)\n",
        "    def __getitem__(self, i):\n",
        "        s = self.src[i][:self.max_len]; t = self.tgt[i][:self.max_len]\n",
        "        return torch.tensor(s), torch.tensor(t)\n",
        "\n",
        "def collate(batch):\n",
        "    srcs, tgts = zip(*batch)\n",
        "    max_s=max(len(x) for x in srcs); max_t=max(len(x) for x in tgts)\n",
        "    S=torch.full((len(batch),max_s), PAD, dtype=torch.long)\n",
        "    T=torch.full((len(batch),max_t), PAD, dtype=torch.long)\n",
        "    for i,(s,t) in enumerate(zip(srcs,tgts)): S[i,:len(s)]=s; T[i,:len(t)]=t\n",
        "    return S,T\n",
        "\n",
        "# --- load test data ---\n",
        "test_ds = NMTDataset(DATA/\"test.src.ids\", DATA/\"test.tgt.ids\", max_len=128)\n",
        "test_ld = DataLoader(test_ds, batch_size=64, shuffle=False, collate_fn=collate)\n",
        "\n",
        "# --- build model & load checkpoint ---\n",
        "# ⚠️ adapt emb/hid/layers to match the training config you used\n",
        "model = Seq2Seq(src_vocab, tgt_vocab, emb=256, hid=512,\n",
        "                enc_layers=2, dec_layers=4, dropout=0.3).to(device)\n",
        "\n",
        "ckpt = torch.load(CKPT, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model.eval()\n",
        "\n",
        "# --- detokenize (target side) ---\n",
        "tgt_itos = json.load(open(DATA/\"bpe_ro.json\", encoding=\"utf-8\"))[\"itos\"]\n",
        "def ids_to_text(ids):\n",
        "    toks = [tgt_itos[i] if i < len(tgt_itos) else \"<unk>\" for i in ids]\n",
        "    toks = [t for t in toks if t not in [\"<pad>\",\"<bos>\",\"<eos>\",\"<unk>\"]]\n",
        "    s=\"\"\n",
        "    for t in toks:\n",
        "        s += \" \"+t[1:] if t.startswith(\"▁\") else t\n",
        "    return s.strip()\n",
        "\n",
        "# --- greedy decoding ---\n",
        "def greedy_decode(src_ids, max_len=128):\n",
        "    src = torch.tensor([src_ids], dtype=torch.long, device=device)\n",
        "    with torch.no_grad():\n",
        "        enc_out,(h,c)=model.encoder(src)\n",
        "        H=torch.cat([h[-2],h[-1]],dim=-1); C=torch.cat([c[-2],c[-1]],dim=-1)\n",
        "        dh=model.bridge_h(H).unsqueeze(0).repeat(model.decoder.rnn.num_layers,1,1)\n",
        "        dc=model.bridge_c(C).unsqueeze(0).repeat(model.decoder.rnn.num_layers,1,1)\n",
        "        y=torch.tensor([[BOS]], dtype=torch.long, device=device)\n",
        "        out=[]\n",
        "        for _ in range(max_len):\n",
        "            logits,(dh,dc)=model.decoder(y,(dh,dc))\n",
        "            nid=logits[:,-1,:].argmax(dim=-1).item()\n",
        "            if nid==EOS: break\n",
        "            out.append(nid)\n",
        "            y=torch.cat([y, torch.tensor([[nid]], device=device)], dim=1)\n",
        "    return out\n",
        "\n",
        "# --- test perplexity ---\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD)\n",
        "def test_loss_perplexity():\n",
        "    model.eval()\n",
        "    total, steps = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in test_ld:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            inp  = tgt[:, :-1]\n",
        "            gold = tgt[:, 1:].contiguous().view(-1)\n",
        "            logits = model(src, inp).contiguous().view(-1, tgt_vocab)\n",
        "            loss = criterion(logits, gold)\n",
        "            total += float(loss); steps += 1\n",
        "    avg = total / max(steps,1)\n",
        "    ppl = math.exp(min(avg, 20))\n",
        "    return avg, ppl\n",
        "\n",
        "# --- full evaluation ---\n",
        "test_src_ids = read_ids(DATA/\"test.src.ids\")\n",
        "test_tgt_ids = read_ids(DATA/\"test.tgt.ids\")\n",
        "\n",
        "hyps, refs = [], []\n",
        "for s_ids, t_ids in zip(test_src_ids, test_tgt_ids):\n",
        "    hyp_ids = greedy_decode(s_ids)\n",
        "    hyps.append(ids_to_text(hyp_ids))\n",
        "    refs.append(ids_to_text([i for i in t_ids if i not in (PAD,BOS,EOS)]))\n",
        "\n",
        "bleu = sacrebleu.corpus_bleu(hyps, [refs]).score\n",
        "\n",
        "cer_sum, chars = 0, 0\n",
        "for h, r in zip(hyps, refs):\n",
        "    cer_sum += Levenshtein.distance(h, r)\n",
        "    chars   += max(len(r), 1)\n",
        "cer = 100.0 * cer_sum / max(chars,1)\n",
        "\n",
        "tst_loss, tst_ppl = test_loss_perplexity()\n",
        "print(f\"TEST — BLEU: {bleu:.2f} | CER: {cer:.2f}% | Loss: {tst_loss:.3f} | PPL: {tst_ppl:.2f}\")\n",
        "\n",
        "# --- sample outputs ---\n",
        "print(\"\\nQualitative examples:\")\n",
        "for i in range(min(5, len(hyps))):\n",
        "    print(f\"[{i+1}]\")\n",
        "    print(\"PRED:\", hyps[i])\n",
        "    print(\"REF :\", refs[i])\n",
        "    print(\"---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG3ZOV96hWvR",
        "outputId": "51c2f7db-3c11-4058-a97a-10bf714ca7a0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hTEST — BLEU: 0.49 | CER: 73.14% | Loss: 1.299 | PPL: 3.67\n",
            "\n",
            "Qualitative examples:\n",
            "[1]\n",
            "PRED: maiñ hī nahīñ sar\n",
            "REF : maiñ akelā hī nahīñ barbād sab\n",
            "---\n",
            "[2]\n",
            "PRED: baddā e tujh\n",
            "REF : ba.ad muddat ke ye ai 'dāġh' samajh meñ aayā\n",
            "---\n",
            "[3]\n",
            "PRED: tujh o vālā\n",
            "REF : tam.īz e lāla o gul se hai nāla e bulbul\n",
            "---\n",
            "[4]\n",
            "PRED: mohte ye zarā\n",
            "REF : muñh khole haiñ ye zaḳhm jo bismil ke chaar pāñch\n",
            "---\n",
            "[5]\n",
            "PRED: barbā.eñ\n",
            "REF : barbād e mohabbat kī duā saath liye jā\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "StreamLit"
      ],
      "metadata": {
        "id": "dB5y7RKfJ1Is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "PROJECT = Path(\"/content/drive/MyDrive/25F-7801\")\n",
        "checks = {\n",
        "    \"best_ckpt\" : PROJECT/\"checkpoints\"/\"best.pt\",\n",
        "    \"bpe_ur\"    : PROJECT/\"data\"/\"processed\"/\"bpe_ur.json\",\n",
        "    \"bpe_ro\"    : PROJECT/\"data\"/\"processed\"/\"bpe_ro.json\",\n",
        "    \"train_ids\" : PROJECT/\"data\"/\"processed\"/\"train.src.ids\"\n",
        "}\n",
        "\n",
        "for name, p in checks.items():\n",
        "    print(f\"{name:10s} => {p}  exists? {p.exists()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jn-lD5klklMQ",
        "outputId": "746abbbe-e2d9-4722-a0e4-7abf4598240d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_ckpt  => /content/drive/MyDrive/25F-7801/checkpoints/best.pt  exists? True\n",
            "bpe_ur     => /content/drive/MyDrive/25F-7801/data/processed/bpe_ur.json  exists? True\n",
            "bpe_ro     => /content/drive/MyDrive/25F-7801/data/processed/bpe_ro.json  exists? True\n",
            "train_ids  => /content/drive/MyDrive/25F-7801/data/processed/train.src.ids  exists? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT = Path(\"/content/drive/MyDrive/25F-7801\")\n",
        "DEPLOY = Path(\"/content/deploy_app\")\n",
        "\n",
        "# remove any old deploy dir and create fresh layout\n",
        "if DEPLOY.exists():\n",
        "    shutil.rmtree(DEPLOY)\n",
        "(DEPLOY/\"data\"/\"processed\").mkdir(parents=True, exist_ok=True)\n",
        "(DEPLOY/\"checkpoints\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# source files (from your project)\n",
        "src_bpe_ur = PROJECT/\"data\"/\"processed\"/\"bpe_ur.json\"\n",
        "src_bpe_ro = PROJECT/\"data\"/\"processed\"/\"bpe_ro.json\"\n",
        "best_ckpt   = PROJECT/\"checkpoints\"/\"best.pt\"\n",
        "\n",
        "# copy into deploy folder\n",
        "shutil.copy2(src_bpe_ur, DEPLOY/\"data\"/\"processed\"/\"bpe_ur.json\")\n",
        "shutil.copy2(src_bpe_ro, DEPLOY/\"data\"/\"processed\"/\"bpe_ro.json\")\n",
        "shutil.copy2(best_ckpt,  DEPLOY/\"checkpoints\"/\"best.pt\")\n",
        "\n",
        "# list files in deploy tree\n",
        "print(\"Files in /content/deploy_app/:\")\n",
        "for p in sorted(DEPLOY.rglob(\"*\")):\n",
        "    print(\" \", p.relative_to(DEPLOY))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsXV7Vg4ko5P",
        "outputId": "85fa3ec5-ea1b-4d07-cfeb-0882490c4e4b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in /content/deploy_app/:\n",
            "  checkpoints\n",
            "  checkpoints/best.pt\n",
            "  data\n",
            "  data/processed\n",
            "  data/processed/bpe_ro.json\n",
            "  data/processed/bpe_ur.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "req = \"\"\"\\\n",
        "streamlit\n",
        "torch\n",
        "# optional for eval if needed:\n",
        "# sacrebleu\n",
        "# python-Levenshtein\n",
        "\"\"\"\n",
        "open(\"/content/deploy_app/requirements.txt\",\"w\",encoding=\"utf-8\").write(req)\n",
        "print(\"Wrote requirements.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gm8mapb7ksrg",
        "outputId": "bed3cb6b-a87e-4335-94f0-ed6dca0f57df"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "DEPLOY = Path(\"/content/deploy_app\")\n",
        "APP_PY = DEPLOY/\"app.py\"\n",
        "\n",
        "app_code = r'''\\\n",
        "import json, re, torch\n",
        "import torch.nn as nn\n",
        "import streamlit as st\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------- Paths ----------\n",
        "ROOT = Path(\".\")\n",
        "BPE_DIR = ROOT/\"data\"/\"processed\"\n",
        "CKPT    = ROOT/\"checkpoints\"/\"best.pt\"\n",
        "\n",
        "PAD, BOS, EOS, UNK = 0,1,2,3\n",
        "SPACE = \"▁\"\n",
        "SPECIALS = {\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"}\n",
        "\n",
        "# ---------- BPE utils ----------\n",
        "def _merge_seq(seq, pair):\n",
        "    a,b = pair; out=[]; i=0\n",
        "    while i < len(seq):\n",
        "        if i+1 < len(seq) and seq[i]==a and seq[i+1]==b:\n",
        "            out.append(a+b); i+=2\n",
        "        else:\n",
        "            out.append(seq[i]); i+=1\n",
        "    return out\n",
        "\n",
        "def load_bpe(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def apply_bpe(line, model, add_bos_eos=True):\n",
        "    # simple bpe applicator consistent with your preprocessing (lowercase + split on spaces)\n",
        "    merges = model[\"merges\"]\n",
        "    itos   = model[\"itos\"]\n",
        "    stoi   = {t:i for i,t in enumerate(itos)}\n",
        "    line = re.sub(r\"\\s+\",\" \", line.strip().lower())\n",
        "    pieces=[]\n",
        "    if line:\n",
        "        for w in line.split(\" \"):\n",
        "            seq=[SPACE]+list(w)\n",
        "            for a,b in merges:\n",
        "                seq=_merge_seq(seq,(a,b))\n",
        "            pieces.extend(seq)\n",
        "    ids=[stoi.get(p, UNK) for p in pieces]\n",
        "    return [BOS]+ids+[EOS] if add_bos_eos else ids\n",
        "\n",
        "def ids_to_text(ids, itos):\n",
        "    toks=[itos[i] if 0<=i<len(itos) else \"<unk>\" for i in ids]\n",
        "    toks=[t for t in toks if t not in SPECIALS]\n",
        "    s=\"\"\n",
        "    for t in toks:\n",
        "        s+=(\" \"+t[1:] if t.startswith(SPACE) else t)\n",
        "    return s.strip()\n",
        "\n",
        "# ---------- Model (same architecture you used) ----------\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb=256, hid=512, layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb, padding_idx=PAD)\n",
        "        self.rnn = nn.LSTM(emb, hid//2, num_layers=layers, batch_first=True,\n",
        "                           dropout=dropout if layers>1 else 0.0, bidirectional=True)\n",
        "    def forward(self, x):\n",
        "        emb = self.emb(x)\n",
        "        out, (h,c) = self.rnn(emb)\n",
        "        return out, (h,c)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb=256, hid=512, layers=4, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb, padding_idx=PAD)\n",
        "        self.rnn = nn.LSTM(emb, hid, num_layers=layers, batch_first=True,\n",
        "                           dropout=dropout if layers>1 else 0.0)\n",
        "        self.fc = nn.Linear(hid, vocab_size)\n",
        "    def forward(self, y, state):\n",
        "        emb = self.emb(y)\n",
        "        out, state = self.rnn(emb, state)\n",
        "        return self.fc(out), state\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, src_vocab, tgt_vocab, emb=256, hid=512, enc_layers=2, dec_layers=4, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, emb, hid, enc_layers, dropout)\n",
        "        self.decoder = Decoder(tgt_vocab, emb, hid, dec_layers, dropout)\n",
        "        self.bridge_h = nn.Linear(hid, hid)\n",
        "        self.bridge_c = nn.Linear(hid, hid)\n",
        "\n",
        "    def init_dec(self, h, c):\n",
        "        H = torch.cat([h[-2], h[-1]], dim=-1)\n",
        "        C = torch.cat([c[-2], c[-1]], dim=-1)\n",
        "        dh = self.bridge_h(H).unsqueeze(0).repeat(self.decoder.rnn.num_layers, 1, 1)\n",
        "        dc = self.bridge_c(C).unsqueeze(0).repeat(self.decoder.rnn.num_layers, 1, 1)\n",
        "        return dh, dc\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def greedy(self, src_ids, max_len=128, device=\"cpu\"):\n",
        "        src = torch.tensor([src_ids], dtype=torch.long, device=device)\n",
        "        enc_out, (h,c) = self.encoder(src)\n",
        "        dh, dc = self.init_dec(h, c)\n",
        "        y = torch.tensor([[BOS]], dtype=torch.long, device=device)\n",
        "        out = []\n",
        "        for _ in range(max_len):\n",
        "            logits, (dh, dc) = self.decoder(y, (dh, dc))\n",
        "            nid = logits[:, -1, :].argmax(dim=-1).item()\n",
        "            if nid == EOS:\n",
        "                break\n",
        "            out.append(nid)\n",
        "            y = torch.cat([y, torch.tensor([[nid]], device=device)], dim=1)\n",
        "        return out\n",
        "\n",
        "# ---------- Load artifacts ----------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "src_bpe = load_bpe(BPE_DIR/\"bpe_ur.json\")\n",
        "tgt_bpe = load_bpe(BPE_DIR/\"bpe_ro.json\")\n",
        "tgt_itos = tgt_bpe[\"itos\"]\n",
        "\n",
        "# try to read src/tgt sizes from checkpoint if present, else from bpe files\n",
        "state = torch.load(CKPT, map_location=device)\n",
        "src_vocab = state.get(\"src_vocab\", len(src_bpe[\"itos\"])) if isinstance(state, dict) else len(src_bpe[\"itos\"])\n",
        "tgt_vocab = state.get(\"tgt_vocab\", len(tgt_bpe[\"itos\"])) if isinstance(state, dict) else len(tgt_bpe[\"itos\"])\n",
        "\n",
        "model = Seq2Seq(src_vocab, tgt_vocab).to(device)\n",
        "# load state dict — supports the format {\"model\": state_dict} used earlier\n",
        "if isinstance(state, dict) and \"model\" in state:\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "else:\n",
        "    model.load_state_dict(state)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# ---------- UI ----------\n",
        "st.set_page_config(page_title=\"Urdu → Roman Urdu\", page_icon=\"📝\")\n",
        "st.title(\"Urdu → Roman Urdu Translator\")\n",
        "st.caption(\"BiLSTM Encoder–Decoder with BPE (from scratch) · PyTorch\")\n",
        "\n",
        "inp = st.text_area(\"Urdu input:\", height=140, value=\"میں تم سے محبت کرتا ہوں\")\n",
        "max_len = st.slider(\"Max output length\", 32, 256, 128, step=16)\n",
        "\n",
        "if st.button(\"Translate\"):\n",
        "    if not inp.strip():\n",
        "        st.warning(\"Please enter Urdu text.\")\n",
        "    else:\n",
        "        try:\n",
        "            src_ids = apply_bpe(inp, src_bpe, add_bos_eos=True)\n",
        "            pred_ids = model.greedy(src_ids, max_len=max_len, device=device)\n",
        "            out = ids_to_text(pred_ids, tgt_itos)\n",
        "            st.subheader(\"Roman Urdu\")\n",
        "            st.write(out if out else \"(empty)\")\n",
        "        except Exception as e:\n",
        "            st.error(\"Error during translation: \" + str(e))\n",
        "'''\n",
        "\n",
        "APP_PY.write_text(app_code, encoding=\"utf-8\")\n",
        "print(\"Wrote\", APP_PY)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBc4Fwmhk0Zg",
        "outputId": "61dd9418-7e94-40d7-f86b-1e178404be9f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote /content/deploy_app/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# quick local check (does not start streamlit)\n",
        "import json, torch\n",
        "from pathlib import Path\n",
        "\n",
        "DEPLOY = Path(\"/content/deploy_app\")\n",
        "DATA = DEPLOY/\"data\"/\"processed\"\n",
        "CKPT  = DEPLOY/\"checkpoints\"/\"best.pt\"\n",
        "\n",
        "print(\"Files in deploy:\", list(DATA.iterdir()), CKPT.exists())\n",
        "\n",
        "# load BPEs and model (same logic as app)\n",
        "src_bpe = json.load(open(DATA/\"bpe_ur.json\", encoding=\"utf-8\"))\n",
        "tgt_bpe = json.load(open(DATA/\"bpe_ro.json\", encoding=\"utf-8\"))\n",
        "print(\"BPE sizes:\", len(src_bpe[\"itos\"]), len(tgt_bpe[\"itos\"]))\n",
        "\n",
        "# try to instantiate model and load state\n",
        "import importlib.util, sys\n",
        "# We will run a tiny snippet similar to app.py (reuse the Seq2Seq class by importing the app file is messy),\n",
        "# so just attempt loading the checkpoint dict to ensure it's readable:\n",
        "ck = torch.load(CKPT, map_location=\"cpu\")\n",
        "print(\"Checkpoint keys:\", list(ck.keys()) if isinstance(ck, dict) else \"raw state\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgYwZNAvk4Sv",
        "outputId": "68a3ec6e-65c4-41dd-beac-086c088a8598"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in deploy: [PosixPath('/content/deploy_app/data/processed/bpe_ur.json'), PosixPath('/content/deploy_app/data/processed/bpe_ro.json')] True\n",
            "BPE sizes: 200 200\n",
            "Checkpoint keys: ['model', 'src_vocab', 'tgt_vocab']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "base_dir = Path(\"/content\")\n",
        "deploy_dir = base_dir / \"deploy_app\"\n",
        "zip_path = base_dir / \"app_bundle.zip\"\n",
        "\n",
        "# remove old if exists\n",
        "if zip_path.exists():\n",
        "    zip_path.unlink()\n",
        "\n",
        "# create archive\n",
        "shutil.make_archive(base_name=str(zip_path.with_suffix('')), format=\"zip\",\n",
        "                    root_dir=deploy_dir.parent, base_dir=deploy_dir.name)\n",
        "\n",
        "print(\"✅ Bundle created at:\", zip_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tem46Erkk9iI",
        "outputId": "cada88c7-ad52-4329-ef45-da435123acc0"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Bundle created at: /content/app_bundle.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKCa-Wtu5QiH",
        "outputId": "0e9399ba-def9-4467-e64c-4f08ed562506"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nou2ZlDm6dqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 330zWQS47ftvB2Rr0gmHOYCsNGu_2uV8bk1gEgfkWaeLLHUzQ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLjoee1VoIUG",
        "outputId": "006309e7-52ae-4cf1-d2d0-31033d02bbb1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"🔗 Public URL:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTq6vqK-pb-h",
        "outputId": "41d599cc-5dab-41bb-8a71-ec9a8c199748"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔗 Public URL: NgrokTunnel: \"https://0f5edb5b75be.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.port 8501 &>/dev/null&\n"
      ],
      "metadata": {
        "id": "t_AvVE-Rp-TU"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# 1) Disconnect any existing tunnels\n",
        "for t in ngrok.get_tunnels():\n",
        "    try:\n",
        "        ngrok.disconnect(t.public_url)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# 2) Kill the ngrok agent to reset session\n",
        "ngrok.kill()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVhIXvCg79EK",
        "outputId": "2d31d76b-23b1-464b-ff41-cf638bb485df"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-09-21T15:41:23+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-007cbd2f-2048-4bb4-a31e-cd4b3db34698 acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-09-21T15:41:23+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-9eb86c00-57ed-4a53-91c1-4822ebdc837b acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-09-21T15:41:23+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-78f0cd98-db91-4334-9837-dc89f3348831 acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Start tunnel to the running service on 8501\n",
        "public_url = ngrok.connect(addr=\"http://127.0.0.1:8501\", bind_tls=True)\n",
        "print(\"Public URL:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnAfUUg_qCXF",
        "outputId": "348a389c-11fb-4a08-d7f6-fd8883547207"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://8ae55b3697ae.ngrok-free.app\" -> \"http://127.0.0.1:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/deploy_app/requirements.txt\n",
        "streamlit\n",
        "torch\n",
        "torchvision\n",
        "torchaudio\n",
        "numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZEQHbIhLxby",
        "outputId": "71424f5c-c22a-49cf-c7c0-87857fe8e2bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/deploy_app/requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "73fFbG7LUYrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b0z011ZMUfY3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}